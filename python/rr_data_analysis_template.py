# -*- coding: utf-8 -*-
"""rr_data_analysis_template.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mAI5-eIYz46BE3Q9W6KesuJ7prBGHCNG

Import Libs
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sqlalchemy import create_engine
from scipy import stats
from sklearn.feature_selection import mutual_info_regression
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from statsmodels.tsa.seasonal import seasonal_decompose
from google.colab import files
import io
# -- pip install sqlalchemy-trino

"""Seaborn Settings"""

sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

"""CSV File from PC"""

file_path= None
#file_path_csv=None
#example windows: 'C:\\Users\\user\\project\\data_file.csv'
#example mac: '/home/user/project/data_file.csv'

"""CSV File from Google Colab"""

#load csv file
uploaded=files.upload()

# show upload file
print(uploaded)

# select file from upload
filename = next(iter(uploaded))

# show the data type
print(f"Content Type: {type(uploaded[filename])}")

# create dataframe
df = pd.read_csv(io.BytesIO(uploaded[filename]), engine='python')

#show first rows of the dataframe
df.head()

"""SQL Connection"""

sql_conn=None
#example trino sql: "trino://user@trino-server:8080/hive/default"

"""SQL Query"""

sql_query=None
#example: "SELECT * FROM table LIMIT 10"

"""Load Data"""

def load_data(use_csv=True, file_path='/Users/ronaldo.riley/Downloads/template_test - Sheet1.csv', sql_conn_str=None, sql_query=None):
    if use_csv:
        df = pd.read_csv(file_path, sep=',')
        print("‚úÖ Data loaded from CSV.")
    else:
        engine = create_engine(sql_conn_str)
        df = pd.read_sql_query(sql_query, engine)
        print("‚úÖ Data loaded from SQL.")
    return df

"""Check Data"""

def check_data(df):
    print("\nüîç Missing values per column:")
    print(df.isnull().sum())
    duplicates = df.duplicated().sum()
    print(f"üîç Number of duplicated rows: {duplicates}")

print(df)

"""Clean Data"""

def clean_data(df, remove_duplicates=True, handle_missing='none'):
    if remove_duplicates:
        before = df.shape[0]
        df = df.drop_duplicates()
        after = df.shape[0]
        print(f"‚úÖ Removed {before - after} duplicated rows.")

    if handle_missing == 'drop':
        df = df.dropna()
    elif handle_missing == 'fill_zero':
        df = df.fillna(0)
    elif handle_missing == 'fill_mean':
        numeric_cols = df.select_dtypes(include=np.number).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())
    elif handle_missing == 'fill_median':
        numeric_cols = df.select_dtypes(include=np.number).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())

    return df

"""Exploratory Data Analysis"""

def eda(df):
    print(df.info())
    print(df.describe())
    df.hist(bins=20, figsize=(14, 8))
    plt.suptitle("Distributions of Numeric Variables")
    plt.show()

    sns.boxplot(data=df.select_dtypes(include=np.number))
    plt.title("Boxplot of Numeric Variables")
    plt.show()

eda(df)

"""Correlation Analysis"""

def correlation_analysis(df):
    corr = df.corr(numeric_only=True)
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr, annot=True, cmap='coolwarm')
    plt.title("Correlation Matrix")
    plt.show()

    threshold = 0.7
    corr_pairs = corr.unstack().sort_values(kind="quicksort").drop_duplicates()
    high_corr = corr_pairs[(abs(corr_pairs) > threshold) & (abs(corr_pairs) < 1)]
    print("\nüîó Strong correlations:")
    print(high_corr)

correlation_analysis(df)

"""Trend Analysis"""

def trend_analysis(df, date_col=None, metric_col=None, x=None, y=None):
    if date_col and date_col in df.columns:
        df[date_col] = pd.to_datetime(df[date_col])
        trend = df.groupby(df[date_col].dt.to_period('M')).sum(numeric_only=True)
        trend.index = trend.index.to_timestamp()
        trend.plot()
        plt.title("üìà Trend Over Time")
        plt.xlabel("Date")
        plt.ylabel("Metrics")
        plt.show()

    if x and y and x in df.columns and y in df.columns:
        sns.regplot(data=df, x=x, y=y, line_kws={"color": "red"})
        plt.title(f"üìà Trend: {y} vs {x}")
        plt.show()
        corr_value = df[x].corr(df[y])
        print(f"üîó Correlation between {x} and {y}: {corr_value:.2f}")

trend_analysis(df, date_col='reference_date')

"""Outlier Analysis"""

def outlier_analysis(df):
    z_scores = np.abs(stats.zscore(df.select_dtypes(include=np.number)))
    outliers = (z_scores > 3).sum()
    print(f"üîç Outliers detected per column:\n{outliers}")

outlier_analysis(df)

"""Categorical Distribution"""

def categorical_distribution(df):
    categorical_cols = df.select_dtypes(include='object').columns
    for col in categorical_cols:
        print(f"\nüìä Distribution for {col}:")
        print(df[col].value_counts(normalize=True) * 100)
        sns.countplot(data=df, x=col)
        plt.title(f"Distribution of {col}")
        plt.xticks(rotation=45)
        plt.show()

categorical_distribution(df)

"""Feature Importance"""

def feature_importance(df, target):
    numeric = df.select_dtypes(include=np.number).dropna()
    features = numeric.drop(columns=[target])
    mi = mutual_info_regression(features, numeric[target])
    mi_scores = pd.Series(mi, index=features.columns).sort_values(ascending=False)
    print("\nüìä Feature Importance (Mutual Information):")
    print(mi_scores)
    mi_scores.plot(kind='bar')
    plt.title('Feature Importance (Mutual Information)')
    plt.show()

feature_importance(df,target='attempts')

"""Time Series Decomposition"""

def time_series_decomposition(df, date_col, metric_col, freq='M', period=None):
    df[date_col] = pd.to_datetime(df[date_col])
    ts = df.set_index(date_col).resample(freq).sum(numeric_only=True)[metric_col]

    # Set default periods for each frequency if not provided
    if period is None:
        if freq == 'D':
            period = 7    # weekly seasonality for daily data
        elif freq == 'W':
            period = 52   # yearly seasonality for weekly data (52 weeks)
        elif freq == 'M':
            period = 12   # yearly seasonality for monthly data
        else:
            period = 1    # generic fallback

    min_points = 2 * period  # need at least 2 full cycles

    if len(ts) < min_points:
        print(f"Insufficient data for decomposition: {len(ts)} observations found, minimum required is {min_points}.")
        print("Plotting simple time series instead:")
        ts.plot(marker='o')
        plt.title(f"Time Series of {metric_col} - Frequency '{freq}'")
        plt.show()
    else:
        result = seasonal_decompose(ts, model='additive', period=period)
        result.plot()
        plt.show()

#time_series_decomposition(df, date_col='reference_date', metric_col='fm_plans_purchased')

time_series_decomposition(df, 'reference_date', 'fm_plans_purchased', freq='D')

"""Clustering"""

def clustering_analysis(df, n_clusters=3):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df.select_dtypes(include=np.number))
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(X_scaled)
    df['cluster'] = clusters
    sns.pairplot(df, hue='cluster')
    plt.show()
    return df

clustering_analysis(df, n_clusters=3)

"""Observations"""

# Example usage (uncomment and adjust as needed):
# df = load_data(use_csv=True, file_path='your_file.csv')
# check_data(df)
# df = clean_data(df, remove_duplicates=True, handle_missing='fill_mean')
# eda(df)
# correlation_analysis(df)
# trend_analysis(df, date_col='date', metric_col='sales', x='ad_spend', y='sales')
# outlier_analysis(df)
# categorical_distribution(df)
# feature_importance(df, target='sales')
# time_series_decomposition(df, date_col='date', metric_col='sales')
# df = clustering_analysis(df, n_clusters=3)